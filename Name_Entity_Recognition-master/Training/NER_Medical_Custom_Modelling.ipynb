{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/R-aryan/Name_Entity_Recognition/blob/master/NER_Medical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viEoK2jE0s73"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkZZWACG03TL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXSHW446039Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YGH_DFMs1ODX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import unicodedata\n",
    " \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
    " \n",
    "# Defining Constants\n",
    " \n",
    "# Maximum length of text sentences\n",
    "MAXLEN = 180\n",
    "# Number of LSTM units\n",
    "LSTM_N = 150\n",
    "# batch size\n",
    "BS=48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "z0JkJUI_1QGf",
    "outputId": "5d59afc5-7eb7-4b2f-97bf-5d778e1457b2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Obesity</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>in</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Low-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Middle-Income</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  Doc_ID  Sent_ID           Word tag\n",
       "0   1       1        1        Obesity   O\n",
       "1   2       1        1             in   O\n",
       "2   3       1        1           Low-   O\n",
       "3   4       1        1            and   O\n",
       "4   5       1        1  Middle-Income   O"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/content/drive/My Drive/NER_Disease/train.csv\", encoding=\"latin1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "xOSxuu281Xfi",
    "outputId": "afabcc24-cd32-4bc3-8277-accc2a3b3157"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Doc_ID</th>\n",
       "      <th>Sent_ID</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4543834</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>CCCVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4543835</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4543836</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>MANOVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4543837</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4543838</td>\n",
       "      <td>30001</td>\n",
       "      <td>191283</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  Doc_ID  Sent_ID    Word\n",
       "0  4543834   30001   191283   CCCVA\n",
       "1  4543835   30001   191283       ,\n",
       "2  4543836   30001   191283  MANOVA\n",
       "3  4543837   30001   191283       ,\n",
       "4  4543838   30001   191283      my"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"/content/drive/My Drive/NER_Disease/test.csv\", encoding=\"latin1\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "dgBu6BB01mxV",
    "outputId": "5e1be5d0-5288-4dfe-e1ef-94e7dca55e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O' 'B-indications' 'I-indications']\n"
     ]
    }
   ],
   "source": [
    "print(data['tag'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65JF-IjU1r-t"
   },
   "source": [
    "Creating Word & Tag dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "obtw9dEr1szU",
    "outputId": "687cabf4-7545-4e6c-c1a9-350924862115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of uniques docs, sentences and words in Training set:\n",
      " id         4543833\n",
      "Doc_ID       30000\n",
      "Sent_ID     191282\n",
      "Word        184505\n",
      "tag              3\n",
      "dtype: int64\n",
      "\n",
      "Number of uniques docs, sentences and words in Test set:\n",
      " id         2994463\n",
      "Doc_ID       20000\n",
      "Sent_ID     125840\n",
      "Word        139891\n",
      "dtype: int64\n",
      "\n",
      "Length of vocabulary =  257203\n",
      "\n",
      "number of tags =  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of uniques docs, sentences and words in Training set:\\n\",data.nunique())\n",
    "print(\"\\nNumber of uniques docs, sentences and words in Test set:\\n\",test_data.nunique())\n",
    " \n",
    "# Creating a vocabulary\n",
    "words = list(set(data[\"Word\"].append(test_data[\"Word\"]).values))\n",
    "words.append(\"ENDPAD\")\n",
    " \n",
    "# Converting greek characters to ASCII characters eg. 'naïve café' to 'naive cafe'\n",
    "words = [unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore') for w in words]\n",
    "n_words = len(words)\n",
    "print(\"\\nLength of vocabulary = \",n_words)\n",
    " \n",
    "tags = list(set(data[\"tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(\"\\nnumber of tags = \",n_tags)\n",
    " \n",
    "# Creating words to indices dictionary.\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "# Creating tags to indices dictionary.\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVNyG8UP1xeC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bwodgApe12bR"
   },
   "source": [
    " Getting Train & Test Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvSovfHi13GJ"
   },
   "outputs": [],
   "source": [
    "def get_tagged_sentences(data):\n",
    "\n",
    "\n",
    "    agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(), s[\"tag\"].values.tolist())]\n",
    "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
    "    sentences = [s for s in grouped]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_test_sentences(data):\n",
    "\n",
    "    agg_func = lambda s: [w for w in s[\"Word\"].values.tolist()]\n",
    "    grouped = data.groupby(\"Sent_ID\").apply(agg_func)\n",
    "    sentences = [s for s in grouped]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "rFrDX9oH18SA",
    "outputId": "76e9df31-5070-452b-f160-f0c43a9b0c50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 sentences in a word list format:\n",
      " [[('Obesity', 'O'), ('in', 'O'), ('Low-', 'O'), ('and', 'O'), ('Middle-Income', 'O'), ('Countries', 'O'), (':', 'O'), ('Burden', 'O'), (',', 'O'), ('Drivers', 'O'), (',', 'O'), ('and', 'O'), ('Emerging', 'O'), ('Challenges', 'O'), ('.', 'O')], [('We', 'O'), ('have', 'O'), ('reviewed', 'O'), ('the', 'O'), ('distinctive', 'O'), ('features', 'O'), ('of', 'O'), ('excess', 'O'), ('weight', 'O'), (',', 'O'), ('its', 'O'), ('causes', 'O'), (',', 'O'), ('and', 'O'), ('related', 'O'), ('prevention', 'O'), ('and', 'O'), ('management', 'O'), ('efforts', 'O'), (',', 'O'), ('as', 'O'), ('well', 'O'), ('as', 'O'), ('data', 'O'), ('gaps', 'O'), ('and', 'O'), ('recommendations', 'O'), ('for', 'O'), ('future', 'O'), ('research', 'O'), ('in', 'O'), ('low-', 'O'), ('and', 'O'), ('middle-income', 'O'), ('countries', 'O'), ('(', 'O'), ('LMICs', 'O'), (')', 'O'), ('.', 'O')]]\n"
     ]
    }
   ],
   "source": [
    "# Getting training sentences in a list\n",
    "sentences = get_tagged_sentences(data)\n",
    "print(\"First 2 sentences in a word list format:\\n\",sentences[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "SH5Yf7ee2lLh",
    "outputId": "abed0c37-edf1-48e7-944d-caceba4085d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 sentences in a word list format:\n",
      " [['CCCVA', ',', 'MANOVA', ',', 'my', 'black', 'hen', '.'], ['Comments', 'on', 'repeated', 'measures', '.']]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Getting test sentences in a list\n",
    "test_sentences = get_test_sentences(test_data)\n",
    "print(\"First 2 sentences in a word list format:\\n\",test_sentences[0:2])\n",
    "print(type(test_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "g4RxbFHe2u-t",
    "outputId": "84140551-775b-4e14-c8b1-4c00444a746f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'VPDs', b'0.14-0.87', b'facilitated-diffusion', b'Powassan', b'alpha-monoclonal', b'HHHFNCT', b'decentralize', b'1A135', b'plasmid-derived']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
    "encode('ascii','ignore')] for w in s] for s in test_sentences]\n",
    "\n",
    "print(words[1:10])\n",
    "\n",
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lB1x5xsU28Xr"
   },
   "source": [
    "Feature Extraction for DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4qVyFGw28_S"
   },
   "outputs": [],
   "source": [
    "# Converting words to indices for test sentences (Features)\n",
    "# Converting greek characters to ASCII characters in train set eg. 'naïve café' to 'naive cafe'\n",
    "X = [[word2idx[unicodedata.normalize('NFKD', str(w[0])).\n",
    "encode('ascii','ignore')] for w in s] for s in sentences]\n",
    " \n",
    "# Converting words to indices for test sentences (Features)\n",
    "# Converting greek characters to ASCII characters in test-set eg. 'naïve café' to 'naive cafe'\n",
    "X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).\n",
    "encode('ascii','ignore')] for w in s] for s in test_sentences]\n",
    " \n",
    "'''\n",
    "Padding train and test sentences to 180 words.\n",
    "Sentences of length greater than 180 words are truncated.\n",
    "Sentences of length less than 180 words are padded with a high value.\n",
    "'''\n",
    "X = pad_sequences(maxlen=MAXLEN, sequences=X, padding=\"post\", value=n_words - 1)\n",
    "X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)\n",
    " \n",
    "# Converting tags to indices for test sentences (labels)\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "# Padding tag labels to 180 words.\n",
    "y = pad_sequences(maxlen=MAXLEN, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    " \n",
    "# Making labels in one hot encoded form for DL model\n",
    "y = [to_categorical(i, num_classes=n_tags) for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ei_Rs4LH3AMe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yjcr7qft3GMk"
   },
   "source": [
    "Building  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "colab_type": "code",
    "id": "4r9vQSgJ3Iko",
    "outputId": "8603b50d-5796-4305-b6a5-a6737896a98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 181717 samples, validate on 9565 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "181717/181717 [==============================] - 2320s 13ms/step - loss: 0.0073 - acc: 0.9981 - val_loss: 0.0041 - val_acc: 0.9988\n",
      "Epoch 2/2\n",
      "181717/181717 [==============================] - 2304s 13ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0038 - val_acc: 0.9989\n"
     ]
    }
   ],
   "source": [
    "# 180 dimensional word indices as input\n",
    "input = Input(shape=(MAXLEN,))\n",
    " \n",
    "# Embedding layer of same length output (180 dim embedding will be generated)\n",
    "model = Embedding(input_dim=n_words, output_dim=MAXLEN, input_length=MAXLEN)(input)\n",
    " \n",
    "# Adding dropout layer\n",
    "model = Dropout(0.2)(model)\n",
    " \n",
    "# Bidirectional LSTM to learn from both forward as well as backward context\n",
    "model = Bidirectional(LSTM(units=LSTM_N, return_sequences=True, recurrent_dropout=0.1))(model)\n",
    " \n",
    "# Adding a TimeDistributedDense, to applying a Dense layer on each 180 timesteps\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model) # softmax output layer\n",
    "model = Model(input, out)\n",
    " \n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X, np.array(y), batch_size=BS, epochs=2, validation_split=0.05, verbose=1)\n",
    "\n",
    "model.save('/content/drive/My Drive/NER_Disease/my_NER_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "cQYTpYYa3L4C",
    "outputId": "c22d351d-9ace-46f3-a975-165240346e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 180, 180)          46296540  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 180, 180)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 180, 300)          397200    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 180, 3)            903       \n",
      "=================================================================\n",
      "Total params: 46,694,643\n",
      "Trainable params: 46,694,643\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG_eUjPQHvvd"
   },
   "source": [
    "Prediction on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yBsqju3pjECO"
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8QK5vTqjMlB"
   },
   "outputs": [],
   "source": [
    "model = load_model('/content/drive/My Drive/NER_Disease/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "GneTL0Y9Hwjl",
    "outputId": "f4fa0fcd-323d-4eff-e304-87a4ad1ff5d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities on Test Set:\n",
      " (125840, 180, 3)\n",
      "Predicted tag indices: \n",
      " (125840, 180)\n"
     ]
    }
   ],
   "source": [
    "# Predicting on trained model\n",
    "pred = model.predict(X_test)\n",
    "print(\"Predicted Probabilities on Test Set:\\n\",pred.shape)\n",
    "# taking tag class with maximum probability\n",
    "pred_index = np.argmax(pred, axis=-1)\n",
    "print(\"Predicted tag indices: \\n\",pred_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsHNzq83jLBZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "RDOo3NMFH2GA",
    "outputId": "26b78e03-3c08-499f-e5ef-83c265e231fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of words in Padded test set: 22651200\n",
      "Length of tags in Padded test set: 22651200\n",
      "\n",
      "Check few of words and predicted tags:\n",
      " ['CCCVA', ',', 'MANOVA', ',', 'my', 'black', 'hen', '.', 'ENDPAD', 'ENDPAD'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# Flatten both the features and predicted tags\n",
    "ids,tagids = X_test.flatten().tolist(), pred_index.flatten().tolist()\n",
    " \n",
    "# converting each word indices back to words\n",
    "words_test = [words[ind].decode('utf-8') for ind in ids]\n",
    "# converting each predicted tag indices back to tags\n",
    "tags_test = [tags[ind] for ind in tagids]\n",
    "print(\"Length of words in Padded test set:\",len(words_test))\n",
    "print(\"Length of tags in Padded test set:\",len(tags_test))\n",
    "print(\"\\nCheck few of words and predicted tags:\\n\",words_test[:10],tags_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAnbgA4bOnkr"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('/content/drive/My Drive/NER_Disease/my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "YAj-P5zyPguY",
    "outputId": "e58f31d5-c142-4f98-d5aa-fc3d44997939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of test set words and predicted tags should match.\n",
      "Length of predicted tags: 2994463\n",
      "Length of words in test set: 2994463\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The task here is to convert padded fixed 180 dimensional predicted tags\n",
    "to variable length test set sentences.\n",
    "1. If the sentences have word length shorter than 180,\n",
    "   then predcited tags are skipped.\n",
    "2. If the sentences have word length longer than 180,\n",
    "   then all extra words are tagged with \"O\" tag class.\n",
    "'''\n",
    " \n",
    "i=0\n",
    "j=1\n",
    "predicted_tags = []\n",
    "predicted_words=[]\n",
    "counts = test_data.groupby('Sent_ID')['id'].count().tolist()\n",
    " \n",
    "for index,count in enumerate(counts):\n",
    "    if count <= MAXLEN:\n",
    "        predicted_tags.append(tags_test[i:i+count])\n",
    "        predicted_words.append(words_test[i:i+count])\n",
    "    else:\n",
    "        predicted_tags.append(tags_test[i:i+MAXLEN])\n",
    "        predicted_words.append(words_test[i:i+MAXLEN])\n",
    "        out = ['O']*(count-MAXLEN)\n",
    "        predicted_tags.append(out)\n",
    "        predicted_words.append(out)\n",
    " \n",
    "    i=j*MAXLEN\n",
    "    j=j+1\n",
    " \n",
    "predictions_final = [item for sublist in predicted_tags for item in sublist]\n",
    "words_final = [item for sublist in predicted_words for item in sublist]\n",
    "print(\"\\nLength of test set words and predicted tags should match.\")\n",
    "print(\"Length of predicted tags:\",len(predictions_final))\n",
    "print(\"Length of words in test set:\",test_data['Word'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "colab_type": "code",
    "id": "KtUZiLglPqt-",
    "outputId": "80276fa5-3124-4266-f0e4-a449bd06afa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCVA</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANOVA</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>black</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hen</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Comments</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>on</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>repeated</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>measures</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nikolsky</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sign</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>;</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>page</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        word tag\n",
       "0      CCCVA   O\n",
       "1          ,   O\n",
       "2     MANOVA   O\n",
       "3          ,   O\n",
       "4         my   O\n",
       "5      black   O\n",
       "6        hen   O\n",
       "7          .   O\n",
       "8   Comments   O\n",
       "9         on   O\n",
       "10  repeated   O\n",
       "11  measures   O\n",
       "12         .   O\n",
       "13  Nikolsky   O\n",
       "14      sign   O\n",
       "15         ;   O\n",
       "16      page   O"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv(\"/content/drive/My Drive/NER_Disease/sample_submission.csv\", encoding=\"latin1\")\n",
    "# Creating a dataframe in the submission format\n",
    "df_results = pd.DataFrame({'word':words_final,'tag':predictions_final})\n",
    "# writing csv submission file\n",
    "df_results.to_csv('/content/drive/My Drive/NER_Disease/prediction_final.csv',sep=\",\", index=None)\n",
    "df_results.head(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ShJ9GBNYTZ0j"
   },
   "outputs": [],
   "source": [
    "# def predict_output(X_test,words):\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "#   # Predicting on trained model\n",
    "#   pred = model.predict(X_test)\n",
    "#   print(\"Predicted Probabilities on Test Set:\\n\",pred.shape)\n",
    "#   # taking tag class with maximum probability\n",
    "#   pred_index = np.argmax(pred, axis=-1)\n",
    "#   print(\"Predicted tag indices: \\n\",pred_index.shape)\n",
    "\n",
    "\n",
    "#   # Flatten both the features and predicted tags\n",
    "#   ids,tagids = X_test.flatten().tolist(), pred_index.flatten().tolist()\n",
    " \n",
    "#   # converting each word indices back to words\n",
    "#   words_test = [words[ind].decode('utf-8') for ind in ids]\n",
    "#   # converting each predicted tag indices back to tags\n",
    "#   tags_test = [tags[ind] for ind in tagids]\n",
    "#   #print(tagids)\n",
    "#   print(\"Length of words in Padded test set:\",len(words_test))\n",
    "#   print(\"Length of tags in Padded test set:\",len(tags_test))\n",
    "#   print(\"\\nCheck few of words and predicted tags:\\n\",words_test[:10],tags_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VTP6J_AzZ2K"
   },
   "outputs": [],
   "source": [
    "# def convert_input(str_test):\n",
    "\n",
    "\n",
    "#   # doc = nlp(str_test)\n",
    "#   # print(doc.ents)\n",
    " \n",
    "\n",
    "#   str_test = list(str_test.split())\n",
    " \n",
    "\n",
    "#   words= list(set(str_test))\n",
    "\n",
    "#   words.append(\"ENDPAD\")\n",
    "\n",
    "#   n_words = len(words)\n",
    "\n",
    "#   words= [unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore') for w in words]\n",
    "\n",
    "#   # Creating words to indices dictionary.\n",
    "#   word2idx = {w: i for i, w in enumerate(words)}\n",
    "\n",
    "#   X_test = [[word2idx[unicodedata.normalize('NFKD', str(w)).encode('ascii','ignore')] for w in str_test]]\n",
    "\n",
    "#   X_test = pad_sequences(maxlen=MAXLEN, sequences=X_test, padding=\"post\", value=n_words - 1)\n",
    "\n",
    "#   predict_output(X_test,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8Ib6e0jzqji"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pqGoWUomzwPJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM6f+wycHxrwqj6flT7LG3H",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NER_Medical",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
